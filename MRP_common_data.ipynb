{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:38.589830Z",
     "start_time": "2018-03-11T05:33:37.519899Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from misc import *\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "from scipy.special import logit\n",
    "from theano import shared\n",
    "from itertools import product\n",
    "\n",
    "import pymc3 as pm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:39.993261Z",
     "start_time": "2018-03-11T05:33:38.591355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Alabama', 122],\n",
       "       ['Alaska', 3],\n",
       "       ['Arizona', 110],\n",
       "       ['Arkansas', 37],\n",
       "       ['California', 536],\n",
       "       ['Colorado', 52],\n",
       "       ['Connecticut', 44],\n",
       "       ['Delaware', 9],\n",
       "       ['District of Columbia', 33],\n",
       "       ['Florida', 380],\n",
       "       ['Georgia', 196],\n",
       "       ['Hawaii', 10],\n",
       "       ['Idaho', 22],\n",
       "       ['Illinois', 286],\n",
       "       ['Indiana', 105],\n",
       "       ['Iowa', 59],\n",
       "       ['Kansas', 36],\n",
       "       ['Kentucky', 154],\n",
       "       ['Louisiana', 72],\n",
       "       ['Maine', 23],\n",
       "       ['Maryland', 112],\n",
       "       ['Massachusetts', 68],\n",
       "       ['Michigan', 279],\n",
       "       ['Minnesota', 70],\n",
       "       ['Mississippi', 47],\n",
       "       ['Missouri', 111],\n",
       "       ['Montana', 9],\n",
       "       ['Nebraska', 39],\n",
       "       ['Nevada', 36],\n",
       "       ['New Hampshire', 14],\n",
       "       ['New Jersey', 96],\n",
       "       ['New Mexico', 13],\n",
       "       ['New York', 305],\n",
       "       ['North Carolina', 220],\n",
       "       ['North Dakota', 4],\n",
       "       ['Ohio', 331],\n",
       "       ['Oklahoma', 77],\n",
       "       ['Oregon', 57],\n",
       "       ['Pennsylvania', 273],\n",
       "       ['Rhode Island', 20],\n",
       "       ['South Carolina', 64],\n",
       "       ['South Dakota', 11],\n",
       "       ['Tennessee', 137],\n",
       "       ['Texas', 487],\n",
       "       ['Utah', 35],\n",
       "       ['Vermont', 10],\n",
       "       ['Virginia', 153],\n",
       "       ['Washington', 97],\n",
       "       ['West Virginia', 71],\n",
       "       ['Wisconsin', 92],\n",
       "       ['Wyoming', 8]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the survey data\n",
    "\n",
    "index1 = preprocess(pd.read_excel('data/RawData/IN10001.xls'))\n",
    "index4 = preprocess(pd.read_excel('data/RawData/IN10004.xls'))\n",
    "index5 = preprocess(pd.read_excel('data/RawData/IN10005.xls'))\n",
    "index6 = preprocess(pd.read_excel('data/RawData/IN10006.xls'))\n",
    "index7 = preprocess(pd.read_excel('data/RawData/IN10007.xls'))\n",
    "\n",
    "#Create a new dataframe that only contains the demographic information that's included in each survey\n",
    "\n",
    "common_columns = list(set(index1.columns).intersection(set(index7.columns)))\n",
    "\n",
    "common_data = pd.concat([index1[common_columns],\n",
    "                         index4[common_columns],\n",
    "                         index5[common_columns],\n",
    "                         index6[common_columns],\n",
    "                         index7[common_columns]])\n",
    "\n",
    "#Drop the columns that don't contain demographic information\n",
    "\n",
    "common_data.drop(['ID', 'ADID IDFA', 'Time Started', 'Time Finished'], axis=1, inplace=True)\n",
    "\n",
    "#Drop the columns where area is missing since the distributions for each demographic doesn't differ much\n",
    "\n",
    "common_data = common_data.loc[common_data['Area'] != 'Unknown', :]\n",
    "\n",
    "#Get count of respondents by state\n",
    "\n",
    "common_data.groupby(['Area']).size().reset_index().values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:40.417255Z",
     "start_time": "2018-03-11T05:33:39.994790Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode whether or not a respodent will vote in the 2018 House of Representatives election\n",
    "\n",
    "def will_vote(answer):\n",
    "    if answer == \"Won't Vote\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#Encode whether or not a respondent will vote for a major party candidate given that he/she will be voting\n",
    "    \n",
    "def will_vote_major(answer):\n",
    "    if answer == 'Will vote Republican' or answer == 'Will vote Democratic':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#Encode which major party candidate a respondent will vote for given that he/she will be voting for a major party candidate\n",
    "    \n",
    "def which_major(answer):\n",
    "    if answer == 'Will vote Republican':\n",
    "        return 0\n",
    "    elif answer == 'Will vote Democratic':\n",
    "        return 1\n",
    "    \n",
    "#Encode demographic columns and create marginal demographic columns using an overall dataset\n",
    "    \n",
    "def encode_demo(index, common):\n",
    "    #Create separate LabelEncoder instances for each variable\n",
    "\n",
    "    gender_enc = LabelEncoder()\n",
    "    race_enc = LabelEncoder()\n",
    "    age_enc = LabelEncoder()\n",
    "    area_enc = LabelEncoder()\n",
    "    edu_enc = LabelEncoder()\n",
    "    div_enc = LabelEncoder()\n",
    "\n",
    "    race_gender_enc = LabelEncoder()\n",
    "    age_edu_enc = LabelEncoder()\n",
    "    age_gender_enc = LabelEncoder()\n",
    "    edu_gender_enc = LabelEncoder()\n",
    "    \n",
    "    area_gender_enc = LabelEncoder()\n",
    "    area_race_enc = LabelEncoder()\n",
    "    area_age_enc = LabelEncoder()\n",
    "    \n",
    "    #Enumerate the possible values for each variable using the overall dataset\n",
    "    \n",
    "    gender_list = common['Gender'].unique()\n",
    "    age_list = common['Age'].unique()\n",
    "    race_list = common['Race'].unique()\n",
    "    edu_list = common['Education'].unique()\n",
    "    area_list = common['Area'].unique()\n",
    "    div_list = common['US Census Division'].unique()\n",
    "    \n",
    "    #Generate all possible combinations for each pair of variables using the overall dataset\n",
    "    \n",
    "    age_gender_list = [str(marginal) for marginal in product(common['Age'].unique(), common['Gender'].unique())]\n",
    "    age_edu_list = [str(marginal) for marginal in product(common['Age'].unique(), common['Education'].unique())]\n",
    "    race_gender_list = [str(marginal) for marginal in product(common['Race'].unique(), common['Gender'].unique())]\n",
    "    edu_gender_list = [str(marginal) for marginal in product(common['Education'].unique(), common['Gender'].unique())]\n",
    "    \n",
    "    area_gender_list = [str(marginal) for marginal in product(common['Area'].unique(), common['Gender'].unique())]\n",
    "    area_race_list = [str(marginal) for marginal in product(common['Area'].unique(), common['Race'].unique())]\n",
    "    area_age_list = [str(marginal) for marginal in product(common['Area'].unique(), common['Age'].unique())]\n",
    "    \n",
    "    #Fit each LabelEncoder instance to its respective list of values\n",
    "    \n",
    "    gender_enc.fit(gender_list)\n",
    "    race_enc.fit(race_list)\n",
    "    age_enc.fit(age_list)\n",
    "    area_enc.fit(area_list)\n",
    "    edu_enc.fit(edu_list)\n",
    "    div_enc.fit(div_list)\n",
    "    \n",
    "    race_gender_enc.fit(race_gender_list)\n",
    "    age_edu_enc.fit(age_edu_list)\n",
    "    age_gender_enc.fit(age_gender_list)\n",
    "    edu_gender_enc.fit(edu_gender_list)\n",
    "    \n",
    "    area_gender_enc.fit(area_gender_list)\n",
    "    area_race_enc.fit(area_race_list)\n",
    "    area_age_enc.fit(area_age_list)\n",
    "    \n",
    "    #Encode the individual categories as integers\n",
    "\n",
    "    index['Gender Encoded'] = gender_enc.transform(index['Gender'])\n",
    "    index['Race Encoded'] = race_enc.transform(index['Race'])\n",
    "    index['Age Encoded'] = age_enc.transform(index['Age'])\n",
    "    index['Area Encoded'] = area_enc.transform(index['Area'])\n",
    "    index['Education Encoded'] = edu_enc.transform(index['Education'])\n",
    "    index['Division Encoded'] = div_enc.transform(index['US Census Division'])\n",
    "    \n",
    "    #Create new columns for the marginal categories\n",
    "    \n",
    "    def concat(col1, col2):\n",
    "        pair = (col1, col2)\n",
    "        \n",
    "        return str(pair)\n",
    "\n",
    "    index['Race_Gender'] = index[['Race', 'Gender']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    index['Age_Education'] = index[['Age', 'Education']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    index['Age_Gender'] = index[['Age', 'Gender']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    index['Education_Gender'] = index[['Education', 'Gender']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    \n",
    "    index['Area_Gender'] = index[['Area', 'Gender']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    index['Area_Race'] = index[['Area', 'Race']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "    index['Area_Age'] = index[['Area', 'Age']].apply(lambda row: concat(row[0], row[1]), axis=1)\n",
    "\n",
    "    #Encode the marginal categories as integers\n",
    "\n",
    "    index['Race_Gender'] = race_gender_enc.transform(index['Race_Gender'])\n",
    "    index['Age_Education'] = age_edu_enc.transform(index['Age_Education'])\n",
    "    index['Age_Gender'] = age_gender_enc.transform(index['Age_Gender'])\n",
    "    index['Education_Gender'] = edu_gender_enc.transform(index['Education_Gender'])\n",
    "    \n",
    "    index['Area_Gender'] = area_gender_enc.transform(index['Area_Gender'])\n",
    "    index['Area_Race'] = area_race_enc.transform(index['Area_Race'])\n",
    "    index['Area_Age'] = area_age_enc.transform(index['Area_Age'])\n",
    "\n",
    "    return index\n",
    "\n",
    "#Perform one hot encoding on the possible answers for any given question\n",
    "\n",
    "def ohe_question(index, question):\n",
    "    col_name = ' '.join(question.split(' ')[:4])\n",
    "    index = pd.concat([index1,\n",
    "                       pd.get_dummies(index1[question],\n",
    "                                      prefix=col_name,\n",
    "                                      prefix_sep='|')],\n",
    "                      axis=1)\n",
    "    \n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.686864Z",
     "start_time": "2018-03-11T05:33:40.418690Z"
    }
   },
   "outputs": [],
   "source": [
    "common_data['Will Vote'] = common_data['Who will you vote for in the House of Representatives in 2018?'].apply(lambda row: will_vote(row))\n",
    "common_data['Will Vote Major'] = common_data['Who will you vote for in the House of Representatives in 2018?'].apply(lambda row: will_vote_major(row))\n",
    "common_data['Which Major'] = common_data['Who will you vote for in the House of Representatives in 2018?'].apply(lambda row: which_major(row))\n",
    "\n",
    "common_data = encode_demo(common_data, common_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.705719Z",
     "start_time": "2018-03-11T05:33:41.688372Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Group by race, gender, education, and age to see how many are in each cell and how many in each cell will vote\n",
    "\n",
    "common_data_unique = common_data.groupby(['Race Encoded',\n",
    "                                'Gender Encoded',\n",
    "                                'Education Encoded',\n",
    "                                'Age Encoded',\n",
    "                                'Area Encoded',\n",
    "                                'Division Encoded',\n",
    "                                'Race_Gender',\n",
    "                                'Age_Education',\n",
    "                                'Age_Gender',\n",
    "                                'Education_Gender',\n",
    "                                'Area_Gender',\n",
    "                                'Area_Race',\n",
    "                                'Area_Age'])['Will Vote']\n",
    "common_data_unique = common_data_unique.agg([('Will Vote', 'sum'), ('n', 'size')]).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.720234Z",
     "start_time": "2018-03-11T05:33:41.707702Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obtain a mapping from each state to its census division\n",
    "\n",
    "division_map = common_data.groupby(['Area', 'US Census Division', 'Area Encoded', 'Division Encoded']).size().reset_index()[['Area', 'US Census Division', 'Area Encoded', 'Division Encoded']]\n",
    "\n",
    "#Remove duplicate state rows where the census division is missing\n",
    "\n",
    "division_map = division_map.loc[division_map['US Census Division'] != 'Unknown'].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.746202Z",
     "start_time": "2018-03-11T05:33:41.721930Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the 2016 Presidential Election results by state, taken from:\n",
    "#https://www.dailykos.com/stories/2016/11/25/1601042/-Nerd-Alert-This-spreadsheet-contains-every-presidential-election-by-state-from-1828-to-2016\n",
    "\n",
    "\n",
    "state_df = pd.read_csv('demographics/US Presidential Results & PVIs by State 1828-2016 - 2-Party US Pres Results & PVIs.csv',\n",
    "                       header=1, usecols=[0, 2])\n",
    "state_df.columns = ['Area', 'Trump Vote']\n",
    "\n",
    "#Use the same name for District of Columbia in both dataframes\n",
    "\n",
    "state_df.loc[state_df['Area'] == 'Washington DC', 'Area'] = 'District of Columbia'\n",
    "\n",
    "\n",
    "#Join the 2016 Election results with the census divisions\n",
    "\n",
    "state_df = state_df.merge(division_map)\n",
    "\n",
    "#Calculate the log odds of the proportion of each state's voters that voted for Donald Trump in 2016\n",
    "\n",
    "state_trump = logit(state_df['Trump Vote'].values/100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.796906Z",
     "start_time": "2018-03-11T05:33:41.748260Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create Theano constant variables for the demographic and marginal demographic variables\n",
    "\n",
    "age = shared(common_data_unique['Age Encoded'].values)\n",
    "edu = shared(common_data_unique['Education Encoded'].values)\n",
    "race = shared(common_data_unique['Race Encoded'].values)\n",
    "gender = shared(common_data_unique['Gender Encoded'].values)\n",
    "area = shared(common_data_unique['Area Encoded'].values)\n",
    "\n",
    "age_gender = shared(common_data_unique['Age_Gender'].values)\n",
    "race_gender = shared(common_data_unique['Race_Gender'].values)\n",
    "age_edu = shared(common_data_unique['Age_Education'].values)\n",
    "edu_gender = shared(common_data_unique['Education_Gender'].values)\n",
    "\n",
    "area_gender = shared(common_data_unique['Area_Gender'].values)\n",
    "area_race = shared(common_data_unique['Area_Race'].values)\n",
    "area_age = shared(common_data_unique['Area_Age'].values)\n",
    "\n",
    "#Create a Theano constant variable for the number of observations in each population cell\n",
    "\n",
    "n = shared(common_data_unique['n'].values)\n",
    "\n",
    "#Calculate the possible number of distinct marginal demographics\n",
    "\n",
    "n_age_gender = common_data['Age'].unique().size*common_data['Gender'].unique().size\n",
    "n_race_gender = common_data['Race'].unique().size*common_data['Gender'].unique().size\n",
    "n_age_edu = common_data['Age'].unique().size*common_data['Education'].unique().size\n",
    "n_edu_gender = common_data['Education'].unique().size*common_data['Gender'].unique().size\n",
    "\n",
    "n_area_gender = common_data['Area'].unique().size*common_data['Gender'].unique().size\n",
    "n_area_race = common_data['Area'].unique().size*common_data['Race'].unique().size\n",
    "n_area_age = common_data['Area'].unique().size*common_data['Age'].unique().size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:41.804084Z",
     "start_time": "2018-03-11T05:33:41.798577Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the model for the state-level effects coefficient\n",
    "\n",
    "def hierarchical_normal(name, shape, mu=0.0):\n",
    "    delta = pm.Normal('delta_{}'.format(name), 0.0, 1.0, shape=shape)\n",
    "    sigma = pm.HalfCauchy('sigma_{}'.format(name), 5.0)\n",
    "    \n",
    "    return pm. Deterministic(name, mu + delta*sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:44.768457Z",
     "start_time": "2018-03-11T05:33:41.806279Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section creates the state-level variables.\n",
    "===============================================\n",
    "\n",
    "The length of the alpha_division coefficient vector is\n",
    "the number of unique census divisions.\n",
    "The length of the alpha_state coefficient vector is the\n",
    "number of unique state values, excluding \"Unknown\".\n",
    "\n",
    "The alpha_division and alpha_state variables have a mean of 0 and\n",
    "a standard deviation of delta*sigma, where delta is sampled from a\n",
    "normal distribution with a mean of 0 and standard deviation of 1, and\n",
    "sigma is sampled from a half Cauchy distribution with beta = 5.\n",
    "\n",
    "The beta_trump variable is the intercept for the support for Trump in\n",
    "each state in the 2016 election. This variable is normally distributed\n",
    "with a mean of 0 and a standard deviation of 5.\n",
    "\n",
    "The mu_area vector represents the mean for each of the 50 states\n",
    "and Washington DC.\n",
    "The state_trump vector is the log odds of support for Trump in each state\n",
    "in the 2016 election.\n",
    "\"\"\"\n",
    "\n",
    "with pm.Model() as model:\n",
    "    alpha_division = hierarchical_normal(name='region',\n",
    "                                         shape=common_data['Division Encoded'].unique().size)\n",
    "    beta_trump = pm.Normal('trump', 0.0 , 5.0)\n",
    "    mu_area = alpha_division[division_map['Division Encoded'].values] + beta_trump*state_trump\n",
    "    alpha_area = hierarchical_normal(name='area',\n",
    "                                     shape=common_data['Area Encoded'].unique().size,\n",
    "                                     mu=mu_area)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-11T05:33:46.689169Z",
     "start_time": "2018-03-11T05:33:44.770176Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section creates the general population variables.\n",
    "======================================================\n",
    "\n",
    "The intercept beta_0 uses an initial value for sampling that's equal to\n",
    "the log odds of a respondent voting based on the survey responses.\n",
    "\n",
    "Each of the general population demographic and marginal demographic\n",
    "variables have a mean of 0 and a standard deviation of delta*sigma,\n",
    "where delta is sampled from a normal distribution with a mean of 0\n",
    "and standard deviation of 1, and sigma is sampled from a\n",
    "half Cauchy distribution with beta = 5.\n",
    "\n",
    "The variable eta is the multilevel logistic regression model that models\n",
    "the log odds of the voter turnout rate.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with model:\n",
    "    beta_0 = pm.Normal('beta_0', 0.0, 5.0,\n",
    "                       testval=logit(common_data['Will Vote'].mean()))\n",
    "    \n",
    "    alpha_race_gender = hierarchical_normal(name='race_gender',\n",
    "                                            shape=n_race_gender)\n",
    "    alpha_age_gender = hierarchical_normal(name='age_gender',\n",
    "                                           shape=n_age_gender)\n",
    "    alpha_age_edu = hierarchical_normal(name='age_education',\n",
    "                                        shape=n_age_edu)\n",
    "    alpha_edu_gender = hierarchical_normal(name='education_gender',\n",
    "                                           shape=n_edu_gender)\n",
    "    \n",
    "    alpha_area_gender = hierarchical_normal(name='area_gender',\n",
    "                                            shape=n_area_gender)\n",
    "    alpha_area_race = hierarchical_normal(name='area_race',\n",
    "                                          shape=n_area_race)\n",
    "    alpha_area_age = hierarchical_normal(name='area_age',\n",
    "                                         shape=n_area_age)\n",
    "    \n",
    "    alpha_age = hierarchical_normal(name='age',\n",
    "                                    shape=common_data['Age'].unique().size)\n",
    "    alpha_edu = hierarchical_normal(name='education',\n",
    "                                    shape=common_data['Education'].unique().size)\n",
    "    alpha_race = hierarchical_normal(name='race',\n",
    "                                     shape=common_data['Race'].unique().size)\n",
    "    alpha_gender = hierarchical_normal(name='gender',\n",
    "                                       shape=common_data['Gender'].unique().size)\n",
    "    \n",
    "    eta = (beta_0 +\n",
    "           alpha_race_gender[race_gender] + \n",
    "           alpha_age_gender[age_gender] +\n",
    "           alpha_age_edu[age_edu] + \n",
    "           alpha_edu_gender[edu_gender] + \n",
    "           alpha_area_gender[area_gender] + \n",
    "           alpha_area_race[area_race] + \n",
    "           alpha_area_age[area_age] + \n",
    "           alpha_age[age] +\n",
    "           alpha_edu[edu] +\n",
    "           alpha_race[race] + \n",
    "           alpha_gender[gender] + \n",
    "           alpha_area[area])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T05:33:37.549Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the observed random variables using a binomial distribution with a probability equal to eta\n",
    "\n",
    "with model:\n",
    "    p = pm.math.sigmoid(eta)\n",
    "    obs = pm.Binomial('obs', n, p,\n",
    "                      observed=common_data_unique['Will Vote'].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T05:33:37.550Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using advi+adapt_diag...\n"
     ]
    }
   ],
   "source": [
    "#Sample the model using ADVI as the initialization method for the NUTS sampler\n",
    "#(the default setting will cause an error)\n",
    "\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(draws=4000,\n",
    "                      tune=1500,\n",
    "                      nuts_kwargs={'target_accept': 0.99},\n",
    "                      init='advi+adapt_diag',\n",
    "                      n_jobs=10)\n",
    "\n",
    "joblib.dump(trace, 'trace.pkl')\n",
    "print(max(np.max(score) for score in pm.gelman_rubin(trace).values()))\n",
    "pm.energyplot(trace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T05:33:37.552Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "energy = trace['energy']\n",
    "energy_diff = np.diff(energy)\n",
    "sb.distplot(energy - energy.mean(), label='energy')\n",
    "sb.distplot(energy_diff, label='energy diff')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T05:33:37.554Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the trace plot and kernel density plot for each variable\n",
    "\n",
    "model_vars = [beta_0,\n",
    "              alpha_race_gender,\n",
    "              alpha_age_gender,\n",
    "              alpha_age_edu,\n",
    "              alpha_edu_gender,\n",
    "              alpha_area_gender,\n",
    "              alpha_area_race,\n",
    "              alpha_area_age,\n",
    "              alpha_race,\n",
    "              alpha_gender,\n",
    "              alpha_age,\n",
    "              alpha_edu,\n",
    "              alpha_area]\n",
    "\n",
    "for var in model_vars:\n",
    "    pm.traceplot(trace, varnames=[var])\n",
    "    \n",
    "# Display the total number and percentage of divergent transitions\n",
    "\n",
    "divergent = trace['diverging']\n",
    "print('Number of Divergent %d' % divergent.nonzero()[0].size)\n",
    "divperc = divergent.nonzero()[0].size/len(trace)\n",
    "print('Percentage of Divergent %.5f' % divperc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-03-11T05:33:37.556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Perform a posterior predictive check to ensure that the sample distribution overlaps with the dataset mean\n",
    "\n",
    "ppc = pm.sample_ppc(trace, model=model)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist([n.mean() for n in ppc['obs']], bins=19, alpha=0.5)\n",
    "ax.axvline(common_data_unique['Will Vote'].mean())\n",
    "ax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MSR]",
   "language": "python",
   "name": "conda-env-MSR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "notify_time": "30",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
